<html lang="en-US"><head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Personalized News Recommender System | CS_4641_Project</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="Personalized News Recommender System">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://github.gatech.edu/pages/abanerjee312/CS_4641_Project/">
<meta property="og:url" content="https://github.gatech.edu/pages/abanerjee312/CS_4641_Project/">
<meta property="og:site_name" content="CS_4641_Project">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Personalized News Recommender System">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Personalized News Recommender System","name":"CS_4641_Project","url":"https://github.gatech.edu/pages/abanerjee312/CS_4641_Project/"}</script>
<!-- End Jekyll SEO tag -->

    <style class="anchorjs"></style><link rel="stylesheet" href="/pages/abanerjee312/CS_4641_Project/assets/css/style.css?v=c35e4d417e5fd9a324cf5d8056673ea7db992454">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/pages/abanerjee312/CS_4641_Project/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="https://github.gatech.edu/pages/abanerjee312/CS_4641_Project/">CS_4641_Project</a></h1>
      

      <h1 id="personalized-news-recommender-system">Personalized News Recommender System</h1>
<h2 id="introductionbackground">Introduction/Background<a class="anchorjs-link " href="#introductionbackground" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>Personalized news recommendations streamline user experience by saving time, enhancing engagement, and easing information overload (Lavie et al., <a href="https://doi.org/10.1016/j.ijhcs.2009.09.011">User Attitudes Towards News Content Personalization 2010</a>). Content-based filtering suggests articles based on user-viewed content, while collaborative filtering identifies patterns among users with similar behavior (Lavie et al., <a href="https://doi.org/10.1016/j.ijhcs.2009.09.011">User Attitudes Towards News Content Personalization 2010</a>). Benefits of personalized news recommendations include time-saving, increased engagement, and reduced information overload.</p>

<p>Existing works have clustered and categorized headlines based on their events/topics (Tingofurro, <a href="https://github.com/tingofurro/headline_grouping">Tingofurro/headline_grouping: Codebase, Data and Models for the Headline Grouping Paper at NAACL2021</a>). The authors released models designed for classification based on input text/context. Other proposed methods to align with users’ interests use collaborative filtering via profile clustering (Desai, <a href="https://github.com/archd3sai/News-Articles-Recommendation">News Article Recommendation 2020</a>). Content-based filtering suggests articles based on the content that a user has previously viewed or interacted with, while collaborative filtering identifies patterns among users with similar behavior or preferences. Additionally, more sophisticated recommendation methods can cluster by event/topic, profiling techniques (grouping users with similar interests together), or utility (user satisfaction and engagement), among others.</p>

<p>One paper calculates what articles give users more satisfaction out of reading them (Zihayat et al., <a href="https://doi.org/10.1016/j.dss.2018.12.001">A Utility-based News Recommendation System 2019</a>). The study introduced a new utility-based news recommendation system that addresses challenges in understanding user interests using clickstream data, incorporating probabilistic topic models, and outperforming existing methods, as demonstrated through evaluation on a massive real dataset from The Globe and Mail in Canada (Zihayat et al., <a href="https://doi.org/10.1016/j.dss.2018.12.001">A Utility-based News Recommendation System 2019</a>).</p>

<p>The dataset we decided to use was the MIcrosoft News Dataset, or MIND. The MIND is a comprehensive dataset designed for news recommendation research. It consists of approximately 160,000 English news articles and over 15 million impression logs generated by 1 million users (Wu et al., <a href="https://msnews.github.io/assets/doc/ACL2020_MIND.pdf">ACL 2020</a>). The dataset includes detailed information such as title, abstract, body, category, and entities for each news article. Additionally, the impression logs record click events, non-clicked events, and historical news click behaviors of users, all anonymized to protect user privacy (Wu et al., <a href="https://msnews.github.io/assets/doc/ACL2020_MIND.pdf">ACL 2020</a>). Researchers can freely download the dataset for research purposes and access training, validation, and test sets. We used it because of its comprehensiveness, size, and reputability.</p>

<p>Link to dataset: <a href="https://msnews.github.io/">https://msnews.github.io/</a></p>

<p>GitHub repository for the dataset: <a href="https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md">https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md</a></p>

<h2 id="problem-definition">Problem Definition<a class="anchorjs-link " href="#problem-definition" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>Users face challenges in finding news content aligned with their interests amidst an abundance of available articles (Roy et al., <a href="https://www.semanticscholar.org/paper/A-systematic-review-and-research-perspective-on-Roy-Dutta/7f139ac3658ae3f8664f713b0f2af027fa990081">A systematic review and research perspective on recommender systems 2022</a>). Challenges include conflicting reading objectives, information filtering difficulty, and user profile generation (Ulian et al., <a href="doi.org/10.1016/j.eswa.2021.115341">Exploring the Effects of Different Clustering Methods on a News Recommender System 2021</a>). These issues stem from diverse user interests, the overwhelming volume of news articles, and the need for accurate personalization while respecting privacy (Roy et al., <a href="https://www.semanticscholar.org/paper/A-systematic-review-and-research-perspective-on-Roy-Dutta/7f139ac3658ae3f8664f713b0f2af027fa990081">A systematic review and research perspective on recommender systems 2022</a>). Addressing these challenges requires recommender systems that can balance conflicting reading objectives, filter relevant information effectively, and continuously refine user profiles. As such, clustering methods play a crucial role in this process by categorizing users based on behavior and preferences, enabling more tailored news recommendations (Ulian et al., <a href="doi.org/10.1016/j.eswa.2021.115341">Exploring the Effects of Different Clustering Methods on a News Recommender System 2021</a>). Our approach aims to do exactly that, as we are using various methods to determine which is the most effective.</p>

<h2 id="methods">Methods<a class="anchorjs-link " href="#methods" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>We planned to investigate a recommendation system using two unsupervised and one supervised approaches: K-means and DBSCAN clustering and Support Vector Regression. We compare clustering algorithms on title and abstract embeddings of news articles, enabling recommendations by topic/content similarity. We then utilize the Support Vector Regression to predict article click likelihood and rank recommendations to present.</p>

<p>OpenAI offers robust text vectorization and encoding capabilities. This API facilitates semantic similarity calculations, clustering algorithms, and downstream model applications for our recommendation tasks. Sci-kit learn provides the utilities we plan to use.</p>

<p><u>Data Preprocessing Methods</u></p>

<p>Our preprocessing steps for our dataset include filtering bad or NaN values, generating embeddings by calling external APIs, and sampling subsets to generate test and train data. Firstly, we take the MIND’s news dataset, and read the news_id, category, subcategory, title, and abstract columns into a pandas dataframe. Pandas provides an Pythonic interface for managing our tabular data. We combine the title and abstract text for each news article into a single text column, filtering out any rows with missing or empty text fields.</p>

<p>We then generate embeddings for the titles and abstract columns using Azure’s OpenAI API by initializing a client for Azure’s OpenAI service, dividing the text list into smaller chunks of up to 512 text inputs, and calling the OpenAI API’s embeddings.create method. We then took any news item where the abstract was missing (NaN), and filtered the dataset to ensure that the dataset is clean and ready for analysis without missing values that could disrupt the model. Lastly, we saved the preprocessed and enriched dataset, including the title and abstract embeddings, to disk for future use to allow for easy access to the preprocessed data without the need to repeat any preprocessing steps. After preprocessing, we obtain 40k rows of train and test data, containing over 500k samples of click or not-click behavior.</p>

<p><u>Approach for K-means algorithm (Unsupervised Learning)</u></p>

<p>For the K-means model, the concatenated embeddings from the title and abstract were used as input features to cluster the articles. Next, we iterated through a range of cluster numbers, applying the K-means algorithm to the data and calculating the DBI and WCSS for each possible number of clusters. These metrics were then plotted to help determine the most appropriate number of clusters for the data, utilizing the elbow method and the DBI for evaluation.</p>

<p>We selected the K-means algorithm for its efficiency in processing the extensive MIND dataset and its effectiveness in discerning natural groupings based on content similarities. This clustering forms the foundation of our personalized recommendation system, directly aligning with our goal of delivering news tailored to individual user preferences to enhance user engagement. K-means is advantageous for its simplicity and scalability, key attributes for handling large-scale data in user personalization scenarios.</p>

<p>We plan to use the clusters generated to sample similar articles of a certain topic. For example, when known existing clusters, we can choose the best cluster for a set of new articles and process them further. For example, we can select the articles that fell into the ‘sports’ cluster to evaluate click likelihood using our supervised learning models explained below.</p>

<p><u>Logistic Regression and Support Vector Regression (Supervised Learning)</u></p>

<p>We also have implemented a supervised SVR model to predict the likelihood that a user clicks on an article given their past history of articles that they have read. We plan to start with Logistic Regression to predict the likelihood for clicks due to it’s applicability to simple classification tasks between 0 and 1. We then plan to use SVR to predict article popularity, measured by click counts, due to its effectiveness in handling non-linear data and producing accurate predictions even with complex input features. SVR’s ability to minimize error within a specified threshold made it a good model to use to forecast discrete outcomes like click likelihood. This approach lets us rank news recommendations in a personalized manner, prioritizing articles likely to engage users based on their predicted popularity.</p>

<p><u>DBScan clustering algorithm for news recommendation approach</u></p>

<p>In this project, we utilized the DBSCAN clustering algorithm to organize news articles from the MIND dataset, using advanced OpenAI embeddings of their titles and abstracts. The data was first preprocessed and sampled before the pre-computed embeddings were merged into the dataset. These embeddings served as the foundation for DBSCAN, which was applied using a range of epsilon values to determine the optimal setting based on silhouette scores, which we used as a measure of cluster validity.</p>

<p>We chose to use DBSCAN for its robustness in identifying clusters without needing a predefined number of clusters, which is advantageous for dealing with the natural variability in text data. Its ability to handle different data densities and identify outliers ensures that the clusters reflect genuine groupings in the dataset, making it particularly effective for the diverse and complex content within the MIND dataset.</p>

<p>Using the clusters identified by DBSCAN, we can refine our personalized news recommendation system by correlating these clusters with user clicks through cosine similarity. This method enables targeted recommendations by aligning articles from user preferred clusters, enhancing content relevance and discovery. This approach not only recommends articles based on previous interactions but also suggests new, similar content, potentially broadening the user’s engagement with the platform.</p>

<h2 id="results-and-discussion">Results and Discussion<a class="anchorjs-link " href="#results-and-discussion" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p><u>Diagrams for DB Scan</u></p>

<p><img src="https://github.gatech.edu/storage/user/69881/files/387be659-f2b6-4b9b-a90d-6e8b5264ab55" alt="image"></p>

<p>The Epsilon vs. Silhouette Score graph measures the quality of clusters created by DBSCAN at different epsilon values where high silhouette scores indicate well-defined clusters. In our Epsilon vs. Silhouette Score graph, we observe a clear increase in silhouette scores as epsilon values rise, indicating improved cluster quality. The sharp initial dip suggests inadequate clustering at low epsilon values, with improvements as epsilon increases, signifying better-defined clusters. The peak of the graph denotes the optimal epsilon for the most cohesive and distinct clusters. This optimal point will be used to align the DBSCAN clustering with user preferences in the recommendation system.</p>

<p><img src="https://github.gatech.edu/storage/user/69881/files/f7fb4f84-a5fe-45d2-ab2b-e1b90607a358" alt="image"></p>

<p>The DBSCAN clustering visualization, which only shows a single cluster for the MIND dataset, suggests a lack of distinct, separable clusters in the data. This means the current clustering approach is unable to distinguish between different types of articles likely due to excessive noise in the dataset. This could be because the dataset predominantly consists of articles that are closely related or overlapping in content, making it challenging for DBSCAN to differentiate between them.</p>

<p><img src="https://github.gatech.edu/storage/user/69881/files/4e04639b-abab-4a3c-81a5-4b5a0f0faca3" alt="image"></p>

<p>This graph plots the number of clusters v. Davis-Bouldin Index, or DBI. We can see here that as the number of clusters increases, the DBI of the data decreases.</p>

<p><img src="https://github.gatech.edu/storage/user/69881/files/b839c131-7c4a-4933-8251-6d69a9486f07" alt="image"></p>

<p>The elbow graph shows the WCSS vs Number of Clusters of our K-means clustering algorithm and appears to show a general downward linear trend or possibly displaying a very gradual elbow around the 25-30 cluster range. The ambiguity in the graph’s shape suggests that our MIND dataset might not have a clear division between clusters or that distinct groupings require a larger number of clusters to emerge. To clarify this, next steps would involve extending the range of clusters evaluated, potentially investigating higher numbers to see if a more distinct elbow appears.</p>

<p><img src="https://github.gatech.edu/storage/user/69881/files/165fea3d-30e2-4fc6-b6d1-576c2880c730" alt="image"></p>

<p>When running Logistic Regression and Support Vector Regression, we pass the cosine similarity of the previous articles clicked on and the current article to each model. When the articles are similar, we expect cosine similarity to be high, and a user to be more likely to click on the article in general. However, we find that the data does not support this claim. In the graph above, the articles that a user clicked on in average have higher cosine similarity scores, but articles that users do not click on oftentimes also have relatively high similarity scores. This means that cosine similarity is not a good predictor of clicks by itself.</p>

<p>Our current Logistic Regression Model achieves an accuracy of 85% on the train dataset and 79% on the test dataset, but we also suspect that the model is taking advantage of class imbalance.</p>

<p><img src="https://github.gatech.edu/storage/user/69881/files/a2593e99-e0bd-4753-abb7-10387a0060b8" alt="image"></p>

<p>For our results, the K-Means analysis suggests that a cluster count in the low twenties might offer an effective trade-off between cluster distinction and model complexity for the K-means algorithm in our news recommendation system. Next steps for this algorithm would involve validating these clusters against user interaction data to ensure they align with user preferences and refining the model accordingly.</p>

<p>For the DBSCAN clustering, rising silhouette scores with larger epsilon values indicate better clustering, yet a single cluster result reflects the MIND dataset’s content homogeneity. The consistent outcome of a single cluster across various hyperparameter configurations indicates a limitation in applying this approach to the MIND dataset.</p>

<p>For the SVR, since the TSNE visualization doesn’t have any distinctive visual clusters for the two generated clusters, this approach is also slightly flawed when applied to the MIND dataset.</p>

<p>The elbow method finds the ideal cluster number by assessing variance against cluster count. The silhouette score gauges cluster coherence by assessing similarity within clusters (higher indicating clearer separation). Beta CV assesses clustering stability, comparing the variation within clusters to the total variation. The Davies-Bouldin Index (DBI) quantifies cluster similarity, separability, and compactness.</p>

<p>Our goals included finding the optimal cluster count using the elbow method, achieving high silhouette scores for clear clusters, minimizing beta CV for reliable clustering, evaluating cluster separability with the DBI, and developing a regression model strongly correlated with user engagement and click count. Our expected result is that our multimodal algorithm will recommend news articles uniquely tailored to users, leveraging user history and article popularity with natural language clustering, text embeddings, and regression analysis.</p>

<p>Our KMeans model clustered news articles based on their titles while our DBSCAN model clustered based on abstracts. We wanted to use DBSCAN instead of KMeans for the abstracts because we expected the data to be noisier and more strangely dispersed due to there being a larger amount of text. DBSCAN can be beneficial over KMeans when the clusters can be very differently shaped and sized. However, while we tried our best to adjust hyperparameters for both KMeans and DBSCAN, our KMeans model returned much better results than our DBSCAN model as the noisiness in our vectorized abstracts made the dispersion of data not easily separable by clusters, as shown in the visual above.</p>

<p>Our next steps include vectorizing the text so that it’s less noisy than the current results. We can also evaluate other information used in rating articles (other than just click count), such as time spent on each article or what users search for. We can also use other datasets and other users to test rather than just the MIND dataset (our current user database) to have better accuracy. In the future, we can also try evaluating the effectiveness of other algorithms, other than K-Means, DBSCAN, and SVR. We can also create ground truths for clustering for improved analysis.</p>

<h2 id="references">References<a class="anchorjs-link " href="#references" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>[1] Tingofurro, “Tingofurro/headline_grouping: Codebase, data and models for the headline grouping paper at NAACL2021,” HeadLine Grouping, <a href="https://github.com/tingofurro/headline_grouping">https://github.com/tingofurro/headline_grouping</a> (accessed Feb. 20, 2024).</p>

<p>[2] A. Desai, “News Article Recommendation,” News-Articles-Recommendation, <a href="https://github.com/archd3sai/News-Articles-Recommendation">https://github.com/archd3sai/News-Articles-Recommendation</a> (accessed Feb. 21, 2024).</p>

<p>[3] M. Zihayat, A. Ayanso, X. Zhao, H. Davoudi, and A. An, “A utility-based news recommendation system,” Decision Support Systems, vol. 117, pp. 14–27, Feb. 2019. <a href="https://doi.org/10.1016/j.dss.2018.12.001">doi:10.1016/j.dss.2018.12.001</a></p>

<p>[4] T. Lavie, M. Sela, I. Oppenheim, O. Inbar, and J. Meyer, “User attitudes towards news content personalization,” International Journal of Human-Computer Studies, vol. 68, no. 8, pp. 483–495, Aug. 2010. <a href="https://doi.org/10.1016/j.ijhcs.2009.09.011">doi:10.1016/j.ijhcs.2009.09.011</a></p>

<p>[5] D. Z. Ulian, J. L. Becker, C. B. Marcolin, and E. Scornavacca, “Exploring the effects of different clustering methods on a news recommender system,” Expert Systems with Applications, vol. 183, p. 115341, Nov. 2021. <a href="doi.org/10.1016/j.eswa.2021.115341">doi:10.1016/j.eswa.2021.115341</a></p>

<p>[6] N. Reimers, “Pretrained Models,” Pretrained Models - Sentence-Transformers Documentation, <a href="https://www.sbert.net/docs/pretrained_models.html">https://www.sbert.net/docs/pretrained_models.html</a> (accessed Feb. 21, 2024).</p>

<p>[7] S. Roy and S. K. Dutta, “A systematic review and research perspective on recommender systems,” Semantic Scholar, <a href="https://www.semanticscholar.org/paper/A-systematic-review-and-research-perspective-on-Roy-Dutta/7f139ac3658ae3f8664f713b0f2af027fa990081">https://www.semanticscholar.org/paper/A-systematic-review-and-research-perspective-on-Roy-Dutta/7f139ac3658ae3f8664f713b0f2af027fa990081.</a> (accessed: Feb. 21, 2024)</p>

<p>[8] Wu, F. (2020). Mind: A Large-scale Dataset for News Recommendation. <a href="https://msnews.github.io/assets/doc/ACL2020_MIND.pdf">https://msnews.github.io/assets/doc/ACL2020_MIND.pdf</a></p>

<h2 id="gantt-chart">Gantt Chart<a class="anchorjs-link " href="#gantt-chart" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>
<p><a href="https://gtvault-my.sharepoint.com/:x:/g/personal/ssharma667_gatech_edu/EenmyDrCn9dHuVQ0qe7OsfUB1qmnmhFzupbioVSZB3KgGw?e=13pnWa">Link to Gantt Chart</a></p>

<h2 id="contribution-table">Contribution Table<a class="anchorjs-link " href="#contribution-table" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Proposal Contributions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Nandini Ramakrishnan</td>
      <td>Introduction, problem definition, methods, presentation, recording</td>
    </tr>
    <tr>
      <td>Chandreyi Chakraborty</td>
      <td>Data processing, Kmeans model template, presentation</td>
    </tr>
    <tr>
      <td>Ananth Vivekanand</td>
      <td>SVR visualizations, reflection, recording</td>
    </tr>
    <tr>
      <td>Shivansh Sharma</td>
      <td>Methods, Reflection, Diagrams, presentation</td>
    </tr>
    <tr>
      <td>Arpan Banerjee</td>
      <td>Kmeans and visualizations: elbow graph, Davies–Bouldin index graph, presentation</td>
    </tr>
  </tbody>
</table>

<h2 id="github-repositories">GitHub Repositories<a class="anchorjs-link " href="#github-repositories" aria-label="Anchor" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<ul>
  <li>/kmeans.py: file to run k-means and generate elbow and Davies-Bouldin visualizations.</li>
  <li>/diagrams/kmeans/dbTF-IDF.png: DB vs number of clusters graph with TF-IDF embeddings</li>
  <li>/diagrams/kmeans/db_with_openai_embeddings.png: DB vs number of clusters graph with TF-IDF embeddings, up to 20 clusters</li>
  <li>/diagrams/kmeans/db_with_openai_embeddings2.png: DB vs number of clusters graph with TF-IDF embeddings, up to 30 clusters</li>
  <li>/diagrams/kmeans/elbow_method.png: elbow method visualization</li>
  <li>/examine_MIND_data.ipynb: loads MIND dataset, performs preprocessing, generates embeddings by calling the OpenAI API, and saves the dataframe as a pickle file for easy loading.</li>
  <li>/svr.ipynb: Load the dataset and performs Logistic Regression and Support Vector Regression on the likelihood that an article is clicked given past click behavior.</li>
  <li>/MINDSmall_train/: MIND dataset’s train-set</li>
  <li>/MINDSmall_dev/: MIND dataset’s test-set</li>
  <li>/test1.out/: test clustering results for the original clustering model</li>
  <li>/test1.py/: test file for the original clustering model</li>
  <li>/test.py/: original model’s ipynb file in python form</li>
</ul>



      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  

<div id="hl-aria-live-message-container" aria-live="polite" class="visually-hidden"></div><div id="hl-aria-live-alert-container" role="alert" aria-live="assertive" class="visually-hidden"></div></body></html>
